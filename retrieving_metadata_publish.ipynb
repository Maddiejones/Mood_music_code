{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087f7f4a",
   "metadata": {},
   "source": [
    "# Spotify Extended Streaming History Rehydrator \n",
    "\n",
    "*The aim of this code is to rehydrate the three 'extended streaming histories' gathered as part of the 'Mood Music' dataset: it extracts them from a JSON file, merges and tidies them into a format that matches the yearly streaming data gathered, and then retrieves the artist ids, artist information, and audio features from each of the files from Spotify's API*\n",
    "\n",
    "To run this code you need to change: \n",
    "- The Spotify API client id, secret id, and url \n",
    "- The filepaths (marked as FILEPATH) to match your computer \n",
    "- occasionally, 'XXXX' denotes the three personIDs found at the beginning of the filenames; relevant changes are outlined in the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df325a6-e9bb-49a2-a6c9-8f630b392e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239ee0c",
   "metadata": {},
   "source": [
    "## Merging the JSON files \n",
    "----\n",
    "*Takes the relevant JSON extended streaming history files and adds a personID column, saves as a csv, and merges into one big streaming history file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import every file beginning with the four relevant letters and merge them \n",
    "\n",
    "def merge_json_files(files, output_path):\n",
    "    \"\"\"imports every file specific to one individual and merges them into one file per personID\"\"\"\n",
    "    merged_data = pd.DataFrame()\n",
    "\n",
    "    #loop: iterates through each file in the file list ('files'), reads it in, and adds it to a merged_data fataframe for the personID in question\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            data = pd.read_json(f)\n",
    "            merged_data = pd.concat([merged_data, data], ignore_index=True)\n",
    "\n",
    "    #inserts a column containing the personID based on the filename from which it was imported \n",
    "    merged_data.insert(0, 'personID', files[0][45:49])\n",
    "    # Construct the output file name (based on first file name)\n",
    "    output_file = os.path.join(output_path, files[0][45:59] + '_merged.tsv')\n",
    "    \n",
    "    # Saves the merged data as a csv file \n",
    "    merged_data.to_csv(os.path.join(output_path, output_file), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967916ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import every file beginning with the four relevant letters and merge them \n",
    "def merge_all_csv_files(files, output_path):\n",
    "    \"\"\"imports every streaming file and merges them\"\"\"\n",
    "    merged_data = pd.DataFrame()\n",
    "    for file in files:\n",
    "        data = pd.read_csv(file,sep='\\t')\n",
    "        merged_data = pd.concat([merged_data, data], ignore_index=True)\n",
    "    # Construct the output file name (based on first file name)\n",
    "    output_file = os.path.join(output_path, 'all_merged_with_ids.tsv')\n",
    "\n",
    "    #saves the full merged dataset as a csv file\n",
    "    merged_data.to_csv(os.path.join(output_path, output_file), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1a76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set your input and output filepaths \n",
    "file_path = (r'FILEPATH')\n",
    "output = (r'OUTPUT_FILEPATH')\n",
    "\n",
    "#creates a list of files based on the inputted personID (put one in at a time marked XXXX)\n",
    "files_to_merge = [os.path.join(file_path, f) for f in os.listdir(file_path) if \"XXXX\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd57ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_json_files(files_to_merge, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb35d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_all_csv_files(files_to_merge, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d29603",
   "metadata": {},
   "source": [
    "## Tidying the dataset without metadata\n",
    "-----\n",
    "*Tidying the merged dataset to make it match the information found in the yearly streaming histories*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a1f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports the [above] merged file as untidy_data \n",
    "untidy_data = pd.read_csv(r'FILEPATH\\all_merged_with_ids.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29a950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes and renames relevant columns to make it match yearly dataset \n",
    "def tidy_data(data:pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"takes the merged data and removes/modifies the columns to make it match the yearly datasets from the main streaming histories\"\"\"\n",
    "    singlefile_df = pd.DataFrame(data)\n",
    "\n",
    "    #dropping unnecessary columns\n",
    "    singlefile_df.drop(['username', 'platform', 'conn_country','master_metadata_album_album_name',\n",
    "       'ip_addr_decrypted', 'user_agent_decrypted','reason_start',\n",
    "       'reason_end', 'shuffle', 'skipped', 'offline', 'offline_timestamp',\n",
    "       'episode_name','episode_show_name', 'spotify_episode_uri',\n",
    "       'incognito_mode', 'endTime', 'artistName', 'trackName', 'msPlayed'], axis = 'columns', inplace = True)\n",
    "    \n",
    "    #renaming necessary columns to match yearly streaming data\n",
    "    singlefile_df.rename(columns={'ts':'endTime', 'ms_played':'msPlayed', 'master_metadata_track_name':'trackName',\n",
    "                   'master_metadata_album_artist_name':'artistName','spotify_track_uri':'trackID'\n",
    "                   }, inplace=True)\n",
    "    \n",
    "    #returns tidied dataset\n",
    "    return(singlefile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625b6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_data = tidy_data(untidy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts the endTime column into the correct format for filtering by dates\n",
    "tidy_data['endTime']=pd.to_datetime(tidy_data['endTime'])\n",
    "\n",
    "#filters out any data from before 31/12/22 to make it a similar timeframe as the yearly streaming data \n",
    "filtered_df = tidy_data.loc[(tidy_data['endTime'] >= '2022-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dc3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves out the tidied dataset \n",
    "filtered_df.to_csv(os.path.join(r'FILEPATH', 'merged_and_tidied.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d82e5ec",
   "metadata": {},
   "source": [
    "## Spotify Authentication\n",
    "----\n",
    "*Creates an access token to Spotify's API, allowing you to make API calls for relevant information; must be done before running any code below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "997f6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting environmental variables and authenticating to Spotify using SpotifyClientCredentials [input your Spotify credentials here; for more information on how to create, see here:\n",
    "#https://developer.spotify.com/documentation/web-api/concepts/apps\n",
    "\n",
    "os.environ['SPOTIPY_CLIENT_ID']=\"XXXX\"\n",
    "os.environ['SPOTIPY_CLIENT_SECRET']=\"XXXX\"\n",
    "os.environ['SPOTIPY_REDIRECT_URI']='XXXX'\n",
    "\n",
    "#authenticates to Spotify\n",
    "spotify = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8950e8c",
   "metadata": {},
   "source": [
    "### Testing the token\n",
    "----\n",
    "*Just here to make sure everything is working correctly - the below code should take ~10s to run; if it exceeds 15s, terminate and try authenticating to Spotify again*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde4e01-5c4a-4fd0-baa2-be1ac824a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test block to check token has worked [only run if necessary]; found from the spotipy example page: https://spotipy.readthedocs.io/en/2.24.0/\n",
    "birdy_uri = 'spotify:artist:2WX2uTcsvV5OnS0inACecP'\n",
    "results = spotify.artist_albums(birdy_uri, album_type='album')\n",
    "albums = results['items']\n",
    "while results['next']:\n",
    "    results = spotify.next(results)\n",
    "    albums.extend(results['items'])\n",
    "\n",
    "for album in albums:\n",
    "    print(album['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a068bf28",
   "metadata": {},
   "source": [
    "## Creating the unique_tracks dataframe\n",
    "----\n",
    "*Extracts the unique track ids from the above file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2671206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for creating dataframe containing unique track ids from the extended streaming history; adds in batch identifiers and sorts out the columns\n",
    "tracks_batch_size = 50\n",
    "audio_batch_size = 100\n",
    "\n",
    "def unique_extractor(track_ids:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"extracts the unique tracks from the dataframe and adds in batch identifiers; returns a dataframe containing only unique tracks\"\"\"\n",
    "    track_ids_unique = pd.unique(track_ids['trackID'])\n",
    "    unique_tracks_df = pd.DataFrame({'trackID':track_ids_unique})\n",
    "\n",
    "    #adds in batch identifiers; the limit is 50 for tracks() and 100 for audio_features()\n",
    "    unique_tracks_df['tracks_batch_id'] = 1 + np.floor((np.arange(len(unique_tracks_df)) /tracks_batch_size)).astype(int)\n",
    "    unique_tracks_df['audio_batch_id'] = 1+np.floor((np.arange(len(unique_tracks_df))/audio_batch_size)).astype(int)\n",
    "\n",
    "    #renames column to trackID\n",
    "    unique_tracks_df.rename(columns= {'0':'trackID'})\n",
    "\n",
    "    #creates empty NoneType indicator column for artistID (used later)\n",
    "    unique_tracks_df['artistID']=None\n",
    "\n",
    "    #returns unique_tracks dataframe \n",
    "    return(unique_tracks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0fc79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs the unique extractor function on the merged json extended streaming histories [fill in filepath and filename]\n",
    "track_ids_full = pd.read_csv(r'FILEPATH\\FILENAME.tsv', sep=\"\\t\")\n",
    "unique_tracks_df = unique_extractor(track_ids_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves out the unique tracks dataframe as 'indicator' file \n",
    "unique_tracks_df.to_csv(os.path.join(\".\", \"indicator.tsv\"), sep = \"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402ccc3",
   "metadata": {},
   "source": [
    "## Retrieving Artist Ids\n",
    "----\n",
    "*Set of functions to retrieve the artist ids from the get.tracks() endpoint of spotify's API using the tracks() function from spotipy in batches*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "628e1c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_tracks(indicator:pd.DataFrame) -> list:\n",
    "    \"\"\"creates a batch of 50 trackIDs based on the filled artistID column to send to the API and returns them as a list\"\"\"\n",
    "    #finds the first empty artistID row and creates a batch of 50 track_ids \n",
    "    try:\n",
    "        start_position = indicator['artistID'].last_valid_index() + 1\n",
    "        print(start_position)\n",
    "    \n",
    "    #in case of an error for the first batch of 50\n",
    "    except TypeError:\n",
    "        start_position = 0\n",
    "    end_position = start_position + 50 \n",
    "    print(end_position)\n",
    "\n",
    "    #returns the batch \n",
    "    batch = indicator['trackID'].iloc[start_position:end_position]\n",
    "\n",
    "    #Converts from series to list and removes the na from the series before it is turned into a list \n",
    "    batch = batch.dropna()\n",
    "    batch = batch.to_list()\n",
    "    \n",
    "    return(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a0d7b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_artist_id(track_ids:list) -> pd.DataFrame:\n",
    "    \"\"\"calls the API for track information and extracts the artistID from the dictionary/list it returns\"\"\"\n",
    "    #calls API for track information\n",
    "    track_info = spotify.tracks(track_ids)\n",
    "\n",
    "    #creates empty list\n",
    "    artist_ids = []\n",
    "\n",
    "    #iterates through tracks and appends artist id to artists_ids\n",
    "    for track in tqdm(track_info['tracks']):\n",
    "        track_id = track['uri']\n",
    "        details = {\n",
    "            'trackID': track_id,\n",
    "            'artistID_new': track['artists'][0]['uri']\n",
    "        }\n",
    "        artist_ids.append(details)\n",
    "    \n",
    "    #creates dataframe from artist_ids \n",
    "    return(pd.DataFrame(artist_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d37888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_write_artist_ids():\n",
    "    \"\"\"master function that runs the [above] functions on the indicator file and rewrites it with the retrieved information\"\"\"\n",
    "    #reads in indicator file \n",
    "    indicator = pd.read_csv(os.path.join(\".\", \"indicator.tsv\"), sep = \"\\t\")\n",
    "\n",
    "    #runs above functions to retrieve the batch for the API call and then call the API for the artistID\n",
    "    batch = get_next_tracks(indicator)\n",
    "    new_artists = retrieve_artist_id(batch)\n",
    "\n",
    "    #joins new_artists to indicator file\n",
    "    new_indicator = pd.merge(indicator, new_artists, on='trackID', how='left')\n",
    "    new_indicator['artistID'].fillna(new_indicator['artistID_new'], inplace=True)\n",
    "    new_indicator.drop('artistID_new', axis=1, inplace=True)\n",
    "    \n",
    "    #rewrites the indicator file to include the new retrieved information\n",
    "    new_indicator.to_csv(os.path.join(\".\", \"indicator.tsv\"), sep = \"\\t\", index=False)\n",
    "    \n",
    "    #check if there are more rows to be filled \n",
    "    return(len(new_indicator.index) - new_indicator['artistID'].last_valid_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc75b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs read_write_artist_ids until it fails out, having gathered all of the information in batches according to the above code\n",
    "rowsleft = 10 \n",
    "while rowsleft != 0:\n",
    "    rowsleft = read_write_artist_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d952cca",
   "metadata": {},
   "source": [
    "## Retrieving Artist Info\n",
    "----\n",
    "*Set of functions to retrieve the artist information from the get.artists() endpoint of Spotify's API using the artists() function from spotipy in batches*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139ceadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a new indicator file for the artist info with empty columns, indicator column, and unique artist ids \n",
    "    #reads in indicator \n",
    "indicator = pd.read_csv(os.path.join(\".\", \"indicator.tsv\"), sep = \"\\t\")\n",
    "\n",
    "    #drops any duplicate artistIDs so only left with unique values \n",
    "indicator_unique_artists = indicator.drop_duplicates(subset=['artistID'])\n",
    "\n",
    "    #adds in empty NoneType columns for the information to be retrieved \n",
    "indicator_unique_artists['artistINFO']=None \n",
    "indicator_unique_artists['artist_genre']=None\n",
    "indicator_unique_artists['artist_popularity']=None \n",
    "indicator_unique_artists['name']=None\n",
    "\n",
    "    #saves out as artist_indicator \n",
    "indicator_unique_artists.to_csv(os.path.join(\".\", \"artist_indicator.tsv\"), sep = \"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e2d1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_artists(indicator:pd.DataFrame) -> list:\n",
    "    \"\"\"retrieves next batch of 50 based on the empty artistINFO columns as a list\"\"\"\n",
    "\n",
    "    #finds the first empty artist info row and creates a batch of 50 track_ids \n",
    "    try:\n",
    "        start_position = indicator['artistINFO'].last_valid_index() + 1\n",
    "        print(start_position)\n",
    "\n",
    "    #in case of an error for the first batch of 50\n",
    "    except TypeError:\n",
    "        start_position = 0\n",
    "    end_position = start_position + 50 \n",
    "    print(end_position)\n",
    "\n",
    "    #returns batch between start and end positions defined \n",
    "    batch = indicator['artistID'].iloc[start_position:end_position]\n",
    "\n",
    "    #Converts from series to list and removes the na from the series before it is turned into a list \n",
    "    batch = batch.dropna()\n",
    "    batch = batch.to_list()\n",
    "\n",
    "    #returns the batch list \n",
    "    return(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5281c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_artist_info(artist_ids:list) -> pd.DataFrame:\n",
    "    \"\"\"retrieves the relevant artist information from the API\"\"\"\n",
    "\n",
    "    #retrieves the artist information from the API \n",
    "    artist_info = spotify.artists(artist_ids)\n",
    "\n",
    "    #creates artist_details empty list \n",
    "    artist_details = []\n",
    "\n",
    "    #creates artist_details from the list/dict retrieved by spotify.artists()\n",
    "    for artist in tqdm(artist_info['artists']):\n",
    "        details = { \n",
    "            'artistID': artist['uri'],\n",
    "            'name_new': artist['name'],\n",
    "            'artist_genre_new': artist['genres'],\n",
    "            'artist_popularity_new': artist['popularity'],\n",
    "            'artistINFO_new': 'Retrieved'\n",
    "        }\n",
    "        artist_details.append(details)\n",
    "    \n",
    "    #returns artist_details as a dataframe \n",
    "    return(pd.DataFrame(artist_details))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0175bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_write_artist_info():\n",
    "    \"\"\"runs above functions to gather artistINFO and rewrite artist_indicator with the new information\"\"\"\n",
    "\n",
    "    #imports the artist_indicator\n",
    "    indicator = pd.read_csv(os.path.join(\".\", \"artist_indicator.tsv\"), sep = \"\\t\")\n",
    "\n",
    "    #runs the above functions to retrieve the batch of information from the API \n",
    "    batch = get_next_artists(indicator)\n",
    "    new_artist_info = retrieve_artist_info(batch)\n",
    "\n",
    "    #joins new_artist_info to indicator file\n",
    "    new_indicator = pd.merge(indicator, new_artist_info, on='artistID', how='left')\n",
    "\n",
    "    #fills in correct columns \n",
    "    new_indicator['artistINFO'].fillna(new_indicator['artistINFO_new'], inplace=True)\n",
    "    new_indicator['name'].fillna(new_indicator['name_new'], inplace=True)\n",
    "    new_indicator['artist_genre'].fillna(new_indicator['artist_genre_new'], inplace=True)\n",
    "    new_indicator['artist_popularity'].fillna(new_indicator['artist_popularity_new'], inplace=True)\n",
    "\n",
    "    #drops extra columns \n",
    "    dropped_cols =['artistINFO_new', 'name_new', 'artist_genre_new', 'artist_popularity_new']\n",
    "    new_indicator.drop(dropped_cols, axis=1, inplace=True)\n",
    "    new_indicator.to_csv(os.path.join(\".\", \"artist_indicator.tsv\"), sep = \"\\t\", index=False)\n",
    "\n",
    "    #checks if there are more rows to be filled \n",
    "    return(len(new_indicator.index) - new_indicator['artistINFO'].last_valid_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs read_write_artist_info until it fails out, having gathered all of the information in batches according to the above code\n",
    "rowsleft = 10 \n",
    "while rowsleft != 0:\n",
    "    rowsleft = read_write_artist_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9afb872",
   "metadata": {},
   "source": [
    "## Retrieving Audio Info\n",
    "----\n",
    "*Set of functions to retrieve the audio information from the get.audio.information() endpoint of Spotify's API using the audio_features() function from spotipy in batches*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "df7c2b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the audio_indicator file from indicator\n",
    "    #reads in indicator \n",
    "indicator = pd.read_csv(os.path.join(\".\", \"indicator.tsv\"), sep = \"\\t\")\n",
    "\n",
    "    #adds empty audio info columns into indicator file to create an audio_indicator \n",
    "colnames = ['speechiness', 'danceability', 'tempo', 'energy', 'key', 'loudness', 'mode', \n",
    "            'acousticness', 'instrumentalness', 'liveness', 'valence', 'duration_ms', 'time_signature',\n",
    "            'audioINFO', 'type', 'uri',\t'track_href', 'analysis_url']\n",
    "indicator[colnames]=None\n",
    "\n",
    "    #saves out as audio_indicator \n",
    "indicator.to_csv(os.path.join(\".\", \"audio_indicator.tsv\"), sep = \"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed009afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_audio(indicator:pd.DataFrame) -> list:\n",
    "    \"\"\"defines the next batch of 100 tracks based on the next empty row of audioINFO\"\"\"\n",
    "\n",
    "    #finds the first empty audioINFO row and creates a batch of 100 track_ids\n",
    "    try:\n",
    "        start_position = indicator['audioINFO'].last_valid_index() + 1\n",
    "        print(start_position)\n",
    "    \n",
    "    #in the case of an error in the first iteration\n",
    "    except TypeError:\n",
    "        start_position = 0\n",
    "    end_position = start_position + 100\n",
    "    print(end_position)\n",
    "\n",
    "    #creates the batch\n",
    "    batch = indicator['trackID'].iloc[start_position:end_position]\n",
    "\n",
    "    #Converts from series to list and removes the na from the series before it is turned into a list \n",
    "    batch = batch.dropna()\n",
    "    batch = batch.to_list()\n",
    "\n",
    "    #returns the batch list\n",
    "    return(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4af861fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_audio_info(track_ids:list) -> pd.DataFrame:\n",
    "    \"\"\"retrieves the audio features for the batch inputted\"\"\"\n",
    "    \n",
    "    #calls API for audio information\n",
    "    audio_info = spotify.audio_features(track_ids)\n",
    "\n",
    "    #avoids any NA values triggering the function to stop completely \n",
    "    audio_info = [entry if entry is not None else {} for entry in audio_info]\n",
    "    audio_info_df = pd.DataFrame(audio_info)\n",
    "\n",
    "    #fills in audio info indicator column\n",
    "    audio_info_df['audioINFO']='Retrieved'\n",
    "\n",
    "    #drops/renames columns \n",
    "    audio_info_df.rename(columns={'uri':'trackID'}, inplace=True)\n",
    "    audio_info_df.drop(columns='id', inplace=True)\n",
    "\n",
    "    #returns the audio_info_df\n",
    "    return(audio_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "183b53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_write_audio_info():\n",
    "    \"\"\"runs the above functions on the audio_indicator file and rewrites it with the new (retrievd) information\"\"\"\n",
    "\n",
    "    #imports the audio_indicator file\n",
    "    indicator = pd.read_csv(os.path.join(\".\", \"audio_indicator.tsv\"), sep = \"\\t\") \n",
    "\n",
    "    #runs the above functions to retrieve the audioINFO from the API   \n",
    "    batch=get_next_audio(indicator)\n",
    "    new_audio = retrieve_audio_info(batch)\n",
    "\n",
    "    #merges the indicator dataframe with the retrieved information dataframe, and marks duplicate columns with suffixes _og (for old info) and _new (for new info)\n",
    "    merged_df = pd.merge(indicator, new_audio, on='trackID', how='left', suffixes=('_og', '_new'))\n",
    "\n",
    "    #goes through duplicate columns and keeps the one containing new information, then drops the duplicates\n",
    "    for col in colnames:\n",
    "        merged_df[col] = merged_df[f'{col}_og'].combine_first(merged_df[f'{col}_new'])\n",
    "    merged_df = merged_df.drop(columns=[f'{col}_og' for col in colnames] + \n",
    "                                            [f'{col}_new' for col in colnames])\n",
    "    \n",
    "    #rewrites the audio_indicator file with the new information\n",
    "    merged_df.to_csv(os.path.join(\".\", \"audio_indicator.tsv\"), sep = \"\\t\", index=False)\n",
    "\n",
    "    #checks if there are more batches left\n",
    "    return(len(merged_df.index) - merged_df['audioINFO'].last_valid_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf1091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs read_write_audio_info until it fails out, having gathered all of the information in batches according to the above code\n",
    "rowsleft = 10\n",
    "while rowsleft != 0:\n",
    "    result = read_write_audio_info() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d459e",
   "metadata": {},
   "source": [
    "## Merging artist and audio information\n",
    "----\n",
    "*Tidies up and merges the artist and audio indicators, and combines them back into the full track ids dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "03208227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_audio_artist_merger(audio:pd.DataFrame, artists:pd.DataFrame):\n",
    "    \"\"\"tidies up and merges the audio and artist indicator files based on artistID; produces a unique_metadata file containing all relevant information for unique tracks\"\"\"\n",
    "    \n",
    "    #tidies up audio empty & unecessary columns\n",
    "    audio.drop(columns=['artistINFO', 'artist_genre', 'artist_popularity', 'name'])\n",
    "    artists.drop(columns='trackID', inplace=True)\n",
    "\n",
    "    #merges audio and artist files\n",
    "    master_info_df = pd.merge(audio, artists, on='artistID', how='left', suffixes=('_audio', '_artist'))\n",
    "    #tidies up duplicate columns \n",
    "    master_info_df = master_info_df.drop(columns=[col for col in master_info_df.columns if col.endswith('_audio')])\n",
    "\n",
    "    master_info_df.drop(columns =['tracks_batch_id_artist', 'audio_batch_id_artist', 'uri', 'audioINFO', 'artistINFO_artist'], inplace=True)\n",
    "    master_info_df.rename(columns = {'artist_genre_artist':'artist_genre', 'name_artist':'name', 'artist_popularity_artist':'artist_popularity'}, inplace=True)\n",
    "\n",
    "    #saves out as a file containing artist and audio information for unique tracks \n",
    "    master_info_df.to_csv(os.path.join(\".\", \"unique_metadata.tsv\"), sep = \"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5063469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads in the artists and audio information indicators created above \n",
    "artists = pd.read_csv(os.path.join(\".\", \"artist_indicator.tsv\"), sep = \"\\t\")\n",
    "audio = pd.read_csv(os.path.join(\".\", \"audio_indicator.tsv\"), sep = \"\\t\")\n",
    "\n",
    "#runs the function on the dataframes loaded in above \n",
    "unique_audio_artist_merger(audio, artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "82ed21e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_to_full_df(unique_metadata:pd.DataFrame, track_ids_full:pd.DataFrame):\n",
    "    \"\"\"merges the unique dataset back into the main dataset containing duplicates of the trackIDs based on listening activity\"\"\"\n",
    "\n",
    "    #merges the unique and full dataframes; denotes duplicate columns with the suffixes _og for the full dataset and _retrieved for the unique one\n",
    "    master_metadata = pd.merge(track_ids_full, unique_metadata, on='trackID', how='left', suffixes=('_og', '_retrieved'))\n",
    "\n",
    "    #matches column names to non-extended datasets \n",
    "    master_metadata.drop(columns =['analysis_url', 'track_href'], inplace = True)\n",
    "    master_metadata.rename(columns ={'artist_genre':'genres', 'artist_popularity':'popularity', 'name':'returned_artist'}, inplace = True)\n",
    "    master_metadata['returned_track']=None\n",
    "\n",
    "    #saves out as complete_metadata file \n",
    "    master_metadata.to_csv(os.path.join(\".\", \"complete_metadata.tsv\"), sep = \"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "50c39614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports full track ids dataframe and unique track ids dataframe [remember to replace FILEPATH with your path]\n",
    "track_ids_full = pd.read_csv(r'FILEPATH\\merged_and_tidied.tsv', sep=\"\\t\")\n",
    "unique_metadata = pd.read_csv(os.path.join(\".\", \"unique_metadata.tsv\"), sep = \"\\t\")\n",
    "\n",
    "#runs the above function to create the complete_metadata file \n",
    "back_to_full_df(unique_metadata, track_ids_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2e9492eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports the complete_metadata file \n",
    "master_metadata = pd.read_csv(os.path.join(\".\", \"complete_metadata.tsv\"), sep = \"\\t\")\n",
    "\n",
    "#provides a list of unique personIDs in dataset\n",
    "pd.unique(master_metadata['personID'])\n",
    "\n",
    "#saves out as unique files for each personID input [go through each of the 3 unique personIDs]\n",
    "filtered_df = master_metadata[master_metadata['personID'] =='XXXX'] \n",
    "filtered_df.to_csv(os.path.join(\".\", \"XXXX_hydrated.tsv\"), sep = \"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
